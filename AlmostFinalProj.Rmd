---
title: "Final Project"
author: "Main Writer: Salma Boudra; Editor: Fatima Alanis; Coders: Ryan McLaughlin & Agustin Orozco; Presenters: Alexus Myer & Fatima Alanis"
date: "2024-04-30"
output:
  pdf_document: default
  
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
  
```
## Lying Statistics with Student Performance Data
### Introduction

The dataset features students' performance and how it is affected by variables like gender, race/ethnicity, parental level of education, lunch, and test preparation. Gender is divided into two categories: male and female. Race/ethnicity is broken down into groups A, B, C, D, and E. Parental level of education is categorized by degrees: bachelor’s, some college, master’s, associate’s, and high school. Lunch refers to whether the students received standard lunch or free/reduced lunch before the test. Test preparation refers to whether or not they completed the test preparation course before the test. Regarding tests, this study examined math scores, reading scores, and writing scores. The purpose of this paper is to examine the student performance dataset and demonstrate how easily statistics can be manipulated. Lying in statistics refers to presenting research results in a manner that could mislead readers into misinterpreting them. For example, the results of a study meant to show how effective a diet pill is may claim it works when it doesn't, using techniques like p-hacking. P-hacking involves running multiple models and selecting only the one that shows a significant result, then publishing it without mentioning the other tests that did not show significance (Class notes). Another method could involve selectively including outliers that significantly impact the results (Class notes). Another method of misleading readers would be to remove any covariates that render the results insignificant (Class notes). These are just a few examples of how results can be manipulated to achieve desired outcomes. This paper aims to illustrate various methods of deception using our student performance dataset. Additionally, we will demonstrate the correct code used to counteract deceptive practices.

### Results:
## Incorrect code: Test Preparation Course Completion vs Test Scores

In the first part of the results, we started off by looking at how to lie with the relationship between test preparation course completion data and test scores.

```{r}
library(tidyverse)
library(boot)
```
```{r}
STUDY <- read.csv("/Users/fatimaalanis/Desktop/lyingstats/study_performance.csv")

head(STUDY)
```

We started off by creating a variable which adds the three different test scores together (math, reading, & writing).

```{r}
STUDY1 <- STUDY %>%
  mutate(total_score = math_score + reading_score + writing_score)

head(STUDY1)
```


```{r,echo=FALSE}
##### The echo=FALSE in the r chunk makes this NOT appear in the rmarkdown file when it is knit. This code excludes scores from the parameters listed form the data analysis which is skewing and curating results in our favor.
NONE.COURSE <- STUDY1 %>%
  filter(test_preparation_course == "none" & total_score > 150 & total_score < 250)
  
COMPLETED.COURSE <- STUDY1 %>%
  filter(test_preparation_course == "completed" & total_score > 130 & total_score < 240)

STUDY2 <- bind_rows(NONE.COURSE, COMPLETED.COURSE)
```

Second, we plotted the relationship between the Test Preparation Course and the Total Scores using code that resulted in plot that inaccurately shows the differences in test scores between those who completed the test prepartion course and those who did not. 

```{r}
STUDY2 %>%
  ggplot(aes(x = test_preparation_course, y = total_score)) + 
  geom_point() +
  theme_minimal()

TEST_PREP_SCORES <- STUDY2 %>%
  group_by(test_preparation_course) %>%
  summarize(
    average_total_score = mean(total_score, na.rm = TRUE),
    max_total_score = 300
  )

TEST_PREP_SCORES
```

Our third step for this part of the code, was bootstrapping. 
After bootstrapping, we got a mean of 201.1751 for the group that completed the test preparation course and a mean of 198.7277 for the group that did not. Having a higher mean for the group that completed the course indicates that completing a test preparation course has more of a positive impact on total test scores. 

```{r}
bootstrap <- 1000

resampled_means_completed <- numeric(bootstrap)
resampled_means_none <- numeric(bootstrap)

for (i in 1:bootstrap) {
  resample_indices_completed <- sample(which(STUDY2$test_preparation_course == "completed"), replace = TRUE)
  resample_indices_none <- sample(which(STUDY2$test_preparation_course == "none"), replace = TRUE)
  
  resample_completed <- STUDY2$total_score[resample_indices_completed]
  resample_none <- STUDY2$total_score[resample_indices_none]
  
  resampled_means_completed[i] <- mean(resample_completed)
  resampled_means_none[i] <- mean(resample_none)
}

score.completed <- mean(resampled_means_completed)
score.none <- mean(resampled_means_none)

score.completed
score.none
```

After bootstrapping, a histogram was created for the resampled means of the completed test preparation course group and for the group that did not complete the test preparation course. The histogram below is close to being symmetrical which indicates that the resampled means of the two groups are similar, indicating there are not very significant differences. The overlap between the two groups in the histogram is quite large which also indicates that differences between the two groups are not very significant. 

```{r}
resampled_data <- data.frame(
  group = rep(c("Completed", "None"), each = bootstrap),
  resampled_means = c(resampled_means_completed, resampled_means_none)
)

ggplot(resampled_data, aes(x = resampled_means, fill = group)) +
  geom_histogram(binwidth = 5, position = "identity", alpha = 0.5) +
  geom_vline(xintercept = score.none, linetype = "dashed", color = "black") +
  labs(x = "Resampled Means", y = "Frequency", title = "Distribution of Resampled Means") +
  scale_fill_manual(values = c("Completed" = "blue", "None" = "red")) +
  theme_minimal()
```

After bootstrapping, we then conducted a t-test to look at the relationship between taking the test preparation course and total test scores. Considering the p-value of this t-test is 0.2191, which greater than the p-value of 0.05, we reject the null hypothesis which means that the differences between the group that completed the test preperation course and the group that didn't is not significant. This matches up with the bootstrapping and histogram conclusions.

```{r}
t.test.course <- t.test(total_score ~ test_preparation_course, data = STUDY2)
t.test.course
```

#### Incorrect Code: Lunch type vs Test Scores

After looking at the relationship between completing the test preparation course and total test scores, we decided to look at how lunch type can impact total test scores. The lunch types are broken down into the standard and reduced groups and we began by filtering the datasets for these two groups.

```{r, echo=FALSE}
# The echo=FALSE in the r chunk makes this NOT appear in the rmarkdown file when it is knit. This code excludes scores from the parameters listed form the data analysis which is skewing and curating results in our favor.

LUNCH.STANDARD <- STUDY1 %>%
  filter(lunch == "standard" & total_score > 50 & total_score < 200)
  
LUNCH.REDUCED <- STUDY1 %>%
  filter(lunch == "free/reduced" & total_score > 130 & total_score < 301)

STUDY3 <- bind_rows(LUNCH.STANDARD, LUNCH.REDUCED)
```

We then plotted the relationship between lunch group and the total test score.
From the plot below, it appears as though those who had a free/reduced lunch scored higher than those who had a standard lunch on average. 

```{r}
STUDY3 %>%
  ggplot(aes(x = lunch, y = total_score)) + 
  geom_point() +
  theme_minimal()

LUNCH.SCORES <- STUDY3 %>%
  group_by(lunch) %>%
  summarize(
    average_total_score = mean(total_score, na.rm = TRUE),
    max_total_score = max(total_score, na.rm = TRUE)
  )

LUNCH.SCORES
```

After plotting, we bootstrapped for the two lunch groups. The two resampled means above of 170.5869 for the standard lunch group test scores and 195.0309 for the free/reduced lunch group test scores show that the second group had a higher mean. This indicates that those who had a free/reduced lunch scored better than those who had a standard lunch.

```{r}
bootstrap <- 1000

resampled_means_standard <- numeric(bootstrap)
resampled_means_free_reduced <- numeric(bootstrap)

for (i in 1:bootstrap) {
  resample_indices_standard <- sample(which(STUDY3$lunch == "standard"), replace = TRUE)
  resample_indices_free_reduced <- sample(which(STUDY3$lunch == "free/reduced"), replace = TRUE)
  
  resample_standard <- STUDY3$total_score[resample_indices_standard]
  resample_free_reduced <- STUDY3$total_score[resample_indices_free_reduced]
  
  resampled_means_standard[i] <- mean(resample_standard)
  resampled_means_free_reduced[i] <- mean(resample_free_reduced)
}

score_standard <- mean(resampled_means_standard)
score_free_reduced <- mean(resampled_means_free_reduced)

score_standard
score_free_reduced
```

After bootstrapping, we plotted a histogram to look at the two lunch groups and their total test scores. In the histogram below, the histograms for both the reduced/free lunch students and the standard lunch students, appear to be quite similar with the standard students' histogram having a higher frequency. However, neither of the groups overlapped on the histogram which indicates that there are significant differences between the two groups in terms of total test scores.

```{r}
resampled_data <- data.frame(
  group = rep(c("Standard", "Free/Reduced"), each = bootstrap),
  resampled_means = c(resampled_means_standard, resampled_means_free_reduced)
)

ggplot(resampled_data, aes(x = resampled_means, fill = group)) +
  geom_histogram(binwidth = 5, position = "identity", alpha = 0.7) +
  labs(x = "Resampled Means", y = "Frequency", title = "Distribution of Resampled Means by Lunch Type") +
  scale_fill_manual(values = c("Standard" = "blue", "Free/Reduced" = "red")) +
  theme_minimal()
```

Our next step to further look at the differences between the two groups in terms of their total test scores was to perform t-tests. The t-test yielded a p-value of 2.2 * 10^-16 which shows high significance. This indicates the differences between the test scores of the two groups are significant.

```{r}
t.test.lunch <- t.test(total_score ~ lunch, data = STUDY3)
t.test.lunch
```

### Correct code: Test Preparation Course Completion vs Test Scores

We revisted the code used above but this time, we corrected it to give accurate and unmanipulated results. We again started by plotting the relationship between taking the test preparation course and the total test score. This plot shows that those who completed the test preparation course and those who did not were both able to score high scores on average with there being a bigger range of overall test scores for the group that did not complete the course. 

```{r}
unique(STUDY1$test_preparation_course) 

STUDY1 %>%
  ggplot(aes(x = test_preparation_course, y = total_score)) + 
  geom_point() +
  theme_minimal()

TRUE_TEST_PREP_SCORES <- STUDY1 %>%
  group_by(test_preparation_course) %>%
  summarize(
    average_total_score = mean(total_score, na.rm = TRUE),
    max_total_score = 300
  )

TEST_PREP_SCORES
```

Similarly to what we previously did, we also moved on to bootstrapping next. Here, the resampled means are very different from the previous ones but the resampled mean of the completed course group is still higher. However, this score is much, much higher than the group that did not complete the course which suggests that there is a bigger difference between the two groups than the incorrect bootstrapping indicated. 

```{r}
bootstrap <- 1000

true.resampled.means.completed <- numeric(bootstrap)
true.resampled.means.none <- numeric(bootstrap)

for (i in 1:bootstrap) {
  true.resample.indices.completed <- sample(which(STUDY1$test_preparation_course == "completed"), replace = TRUE)
  true.resample.indices.none <- sample(which(STUDY1$test_preparation_course == "none"), replace = TRUE)
  
  true.resample.completed <- STUDY1$total_score[true.resample.indices.completed]
  true.resample.none <- STUDY1$total_score[true.resample.indices.none]
  
  true.resampled.means.completed[i] <- mean(true.resample.completed)
  true.resampled.means.none[i] <- mean(true.resample.none)
}

true.score.completed <- mean(true.resampled.means.completed)
true.score.none <- mean(true.resampled.means.none)

true.score.completed
true.score.none
```

For the histogram, we did the same thing as before by plotting the resampled means of the two groups. This histogram showed a higher frequency on average for the group that did not complete the test preparation course but the group that did complete the course has a wider range for resampled means. There is also no overlap in this histogram which indicates that the differences in test scores between the two groups is quite significant. 

```{r}
true_resampled_data_course <- data.frame(
  group = rep(c("Completed", "None"), each = bootstrap),
  true.resampled.means.course = c(true.resampled.means.completed, true.resampled.means.none)
)

ggplot(true_resampled_data_course, aes(x = true.resampled.means.course, fill = group)) +
  geom_histogram(binwidth = 5, position = "identity", alpha = 0.5) +
  geom_vline(xintercept = score.none, linetype = "dashed", color = "black") +
  labs(x = "Resampled Means", y = "Frequency", title = "Distribution of Resampled Means") +
  scale_fill_manual(values = c("Completed" = "blue", "None" = "red")) +
  theme_minimal()
```

Next, a t-test was conducted to look at any significance between the two groups and their test scores. This t-test yielded a p-value of 2.2 * 10^-16 which shows high significance in the differences between the test scores of the two groups.

```{r}
true.test.lunch <- t.test(total_score ~ test_preparation_course, data = STUDY1)
true.test.lunch
```


### Correct code: Lunch type vs Test Scores
We now move on to looking at the results of the correct code for lunch type and test scores. First, we plotted the relationship between the Lunch type and the Total Score. In the plot, the two groups had a pretty similar scoring range with the standard group having higher scores on average.

```{r}
STUDY1 %>%
  ggplot(aes(x = lunch, y = total_score)) + 
  geom_point() +
  theme_minimal()

TRUE.LUNCH.SCORES <- STUDY1 %>%
  group_by(lunch) %>%
  summarize(
    average_total_score = mean(total_score, na.rm = TRUE),
    max_total_score = 300
  )

TRUE.LUNCH.SCORES
```

Next, we bootstrapped. The resampled mean for the group who ate a standard lunch is much higher than the resampled mean for the group that ate a free/reduced lunch. This indicates that there are significant differences in total test scores between the two.

```{r}
bootstrap <- 1000

true_resampled_means_standard <- numeric(bootstrap)
true_resampled_means_free <- numeric(bootstrap)

for (i in 1:bootstrap) {
  true_resample_indices_standard <- sample(which(STUDY1$lunch == "standard"), replace = TRUE)
  true_resample_indices_free <- sample(which(STUDY1$lunch == "free/reduced"), replace = TRUE)
  
  true_resample_standard <- STUDY1$total_score[true_resample_indices_standard]
  true_resample_free <- STUDY1$total_score[true_resample_indices_free]
  
  true_resampled_means_standard[i] <- mean(true_resample_standard)
  true_resampled_means_free[i] <- mean(true_resample_free)
}

true_score_standard <- mean(true_resampled_means_standard)
true_score_free <- mean(true_resampled_means_free)

true_score_standard
true_score_free
```

We then plotted a histogram to further look at differences between the two groups and their test scores.
The histogram has a higher frequency and wider range for the free/reduced group but the fact that there is no overlap between the two groups indicates that there are significant differences between the two groups and their total test scores.

```{r}
true_resampled_data_lunch <- data.frame(
  group = rep(c("Standard", "Free/Reduced"), each = bootstrap),
  true_resampled_means_lunch = c(true_resampled_means_standard, true_resampled_means_free)
)

ggplot(true_resampled_data_lunch, aes(x = true_resampled_means_lunch, fill = group)) +
  geom_histogram(binwidth = 5, position = "identity", alpha = 0.5) +
  geom_vline(xintercept = score.none, linetype = "dashed", color = "black") +
  labs(x = "Resampled Means", y = "Frequency", title = "Distribution of Resampled Means") +
  scale_fill_manual(values = c("Standard" = "blue", "Free/Reduced" = "red")) +
  theme_minimal()
```

The t-test we conducted next also indicated significant differences between the two as the p-value was 2.2*10^-16 which is much smaller than a p-value of 0.05.

```{r}
true.test.lunch <- t.test(total_score ~ lunch, data = STUDY1)
true.test.lunch
```

## Code Part 2
### Accurate Reading Scores vs. Manipulated Reading Scores

In the second part of our results, started off by showing the correct code for calculating median reading score.

```{r}
#Read in data set for student study performance

study_performance <- read.csv("/Users/fatimaalanis/Desktop/lyingstats/study_performance.csv")

#Calculate median reading score
median_score <- median(study_performance$reading_score)

par(mfrow = c(2, 2))
```

After that, we created a histogram of the correct reading scores. This histogram shows a wide range of reading scores and the histogram is not as skewed left.
Next, we started off with creating our first lying code by making a new object that for our fake data which we called "study_performances$lying". We also filled in the top 75% from the reading scores into the new variable.

```{r}
hist(study_performance$reading_score,
     breaks = seq(0, 100, by = 10),
     main = "Breakdown of Reading Scores",
     xlab = "Reading Score",
     ylab = "Frequency",
     col = "skyblue",
     border = "black",
     xlim = c(0, 100),
     ylim = c(0, max(table(cut(study_performance$reading_score, breaks = seq(0, 100, by = 10)))) + 5),
     labels = TRUE)
#add red line to show median
abline(v = median_score, col = "red", lwd = 2)
```

We then started our first lying code with a function to replace the bottom 25% with values from the top 25%. As of right now, everything that was in the bottom percentile is represented by a 1 in the lying fuction. Due to this, we can write code that replaces all the 1s.

```{r}
study_performance$lying <- rep(NA, length(study_performance$reading_score))

fill_bottom_25 <- function(data) {
  quantile_25 <- quantile(data, 0.25)
  data[data <= quantile_25] <- TRUE
  data

  
}

study_performance$lying <- fill_bottom_25(study_performance$reading_score)
```

In this histogram, the purple line showing the median for the new lying data is shifted quite a significant amount to the right of the original median and the histogram overall is skewed more to left than the original histogram. 

```{r}
replace_1_with_top_quartile <- function(data, top_values) {
  set.seed(123)
  top_quartile <- quantile(top_values, 0.75)  # Find the 75th percentile of top_values
  indices_to_replace <- which(data == 1)  # Identify indices where the value is 1
  top_quartile_values <- top_values[top_values >= top_quartile]  # Select values from the top quartile of top_values
  data[indices_to_replace] <- sample(top_quartile_values, length(indices_to_replace), replace = TRUE)  # Replace 1s with random values from the top quartile of top_values
  data
}


study_performance$lying_replaced_with_top_25th_percentile <- replace_1_with_top_quartile(study_performance$lying, study_performance$reading_score)

hist(study_performance$lying_replaced_with_top_25th_percentile,
     breaks = seq(0, 100, by = 10),
     main = "Breakdown of Reading Scores With Lying (top 25 replaces bottom 25)",
     xlab = "Reading Score",
     ylab = "Frequency",
     col = "skyblue",
     border = "black",
     xlim = c(0, 100),
     ylim = c(0, max(table(cut(study_performance$lying_replaced_with_top_25th_percentile, breaks = seq(0, 100, by = 10)))) + 5),
     labels = TRUE)
#add red line to show original median
abline(v = median_score, col = "red", lwd = 2)
#add purple line to show median of new data
abline(v = median(study_performance$lying_replaced_with_top_25th_percentile), col = "purple", lwd =2)

```

Next, lets see what happens if we allow the bottom 25% of scores to be replaced by any value in the top 50% of scores. In this histogram, the purple line showing the median of the new lying data is to the right of the original median and the histogram is once again more skewed to the left than the original histogram.

```{r}
replace_1_with_top_half <- function(data, top_values) {
  set.seed(123)
  top_half <- quantile(top_values, 0.5)  # Find the 50th percentile of top_values
  indices_to_replace <- which(data == 1)  # Identify indices where the value is 1
  top_half_values <- top_values[top_values >= top_half]  # Select values from the top half of top_values
  data[indices_to_replace] <- sample(top_half_values, length(indices_to_replace), replace = TRUE)  # Replace 1s with random values from the top quartile of top_values
  data
}

study_performance$lying_replaced_with_top_50th_percentile <- replace_1_with_top_half(study_performance$lying, study_performance$reading_score)

hist(study_performance$lying_replaced_with_top_50th_percentile,
     breaks = seq(0, 100, by = 10),
     main = "Breakdown of Reading Scores With Lying (top 50 replaces bottom 25)",
     xlab = "Reading Score",
     ylab = "Frequency",
     col = "skyblue",
     border = "black",
     xlim = c(0, 100),
     ylim = c(0, max(table(cut(study_performance$lying_replaced_with_top_50th_percentile, breaks = seq(0, 100, by = 10)))) + 5),
     labels = TRUE)
#add red line to show original median
abline(v = median_score, col = "red", lwd = 2)
#add purple line to show median of new data
abline(v = median(study_performance$lying_replaced_with_top_50th_percentile), col = "purple", lwd =2)

```

Finally, lets see what happens if we allow the bottom 25% to be replaced by any value in the top 75% of scores. For this histogram, the purple line showing the median for the new lying data is shifted to the right of the original median and the histogram as a whole is once again more skewed to the left than the original histogram.

```{r}
replace_1_with_top_75 <- function(data, top_values) {
  set.seed(123)
  top_75 <- quantile(top_values, 0.25)  # Find the 25th percentile of top_values
  indices_to_replace <- which(data == 1)  # Identify indices where the value is 1
  top_75_values <- top_values[top_values >= top_75]  # Select values from the top 75% of top_values
  data[indices_to_replace] <- sample(top_75_values, length(indices_to_replace), replace = TRUE)  # Replace 1s with random values from the top quartile of top_values
  data
}

study_performance$lying_replaced_with_top_75th_percentile <- replace_1_with_top_75(study_performance$lying, study_performance$reading_score)

hist(study_performance$lying_replaced_with_top_75th_percentile,
     breaks = seq(0, 100, by = 10),
     main = "Breakdown of Reading Scores With Lying (top 75 replaces bottom 25)",
     xlab = "Reading Score",
     ylab = "Frequency",
     col = "skyblue",
     border = "black",
     xlim = c(0, 100),
     ylim = c(0, max(table(cut(study_performance$lying_replaced_with_top_75th_percentile, breaks = seq(0, 100, by = 10)))) + 5),
     labels = TRUE)
#add red line to show original median
abline(v = median_score, col = "red", lwd = 2)
#add purple line to show median of new data
abline(v = median(study_performance$lying_replaced_with_top_75th_percentile), col = "purple", lwd =2)

```

### Conclusion
Lying in coding is still lying, whether intentional or not, and like all lying, there are consequences. There can be major consequences, such as ethical ones that may lead to law-breaking, or economic ones, where events like stock market crashes or even recessions can occur. There can even be consequences like the loss of reputation, where trust islost, and people no longer rely on you or your code. A consequence could also be as small as encountering ads everywhere that falsely advertise a product you buy, only for it to end up harming you more than helping. In our paper, we often observed this phenomenon with the test preparation course groups in relation to test scores, as well as the outcomes when certain values in the data were replaced with others. For example, in the test preparation course versus test scores code of our first half, we changed the way we plotted data, bootstrapped, and conducted t-tests. Consequently, we inaccurately compared the test scores of the two preparation course groups. The same issue arose in the lunch type versus test scores code. Moving on to the second part of our analysis, we initially presented the correct median reading score calculations. Subsequently, we modified the code to manipulate the reading values in three distinct ways: replacing the top 25% with the bottom 25%, swapping the bottom 25% with the top 50%, and substituting the bottom 25% with the top 75%. Overall, our findings were unsurprising: through code manipulation and falsehoods, the results diverged significantly from those obtained by correctly executing the code.
